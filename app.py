# -*- coding: utf-8 -*-
"""
M√≥dulo Principal da Aplica√ß√£o Flask para Classifica√ß√£o de E-mails.

Esta aplica√ß√£o web permite que o usu√°rio insira o conte√∫do de um e-mail 
ou carregue um arquivo (.txt ou .pdf) para classifica√ß√£o. 
O processo de classifica√ß√£o pode ser executado em tr√™s modos:
1. 'local': Usando um modelo de Machine Learning treinado previamente (e-mail_classifier.joblib).
2. 'openai': Usando a API GPT (necessita de chave).
3. 'gemini': Usando a API Gemini (necessita de chave).

O resultado inclui o r√≥tulo da classifica√ß√£o ('Produtivo' ou 'Improdutivo'),
o n√≠vel de confian√ßa e uma sugest√£o de resposta autom√°tica.

üîß Melhorias adicionadas (do c√≥digo do GPT):
- Heur√≠sticas extras (acad√™micas, anexos, comunica√ß√£o formal).
- Ajuste do threshold (>=0.6 Produtivo, 0.5‚Äì0.6 zona incerta).
- Respostas contextuais aprimoradas no template_reply.

‚úÖ Melhorias de UX (Experi√™ncia do Usu√°rio) implementadas:
- Bug 1: A sele√ß√£o do modelo de IA agora persiste entre as an√°lises.
- Bug 2: O envio de um arquivo agora tem prioridade sobre o texto digitado na caixa.
- Bug 3: A l√≥gica de backend agora suporta o preview do arquivo no frontend.
"""

# --- Importa√ß√µes de Bibliotecas Essenciais ---
import os           # Para interagir com o sistema operacional (ex: vari√°veis de ambiente, caminhos de arquivo)
import io           # Para trabalhar com streams de I/O em mem√≥ria (necess√°rio para ler arquivos enviados)
import re           # Para express√µes regulares (usado na detec√ß√£o de n√∫meros de pedido/nota)
import json         # Para manipula√ß√£o de dados JSON (usado para comunicar com as APIs de IA)
import traceback    # Para imprimir o stack trace de erros (√∫til para debug)
from flask import Flask, request, render_template # Framework web principal e suas fun√ß√µes

# Biblioteca para carregar o modelo de machine learning local (serializa√ß√£o/desserializa√ß√£o)
import joblib 
# Para ler arquivos PDF e extrair seu texto
import PyPDF2

# --- Importa√ß√µes Opcionais para as APIs de IA ---
# As importa√ß√µes s√£o envolvidas em try/except para que a aplica√ß√£o funcione
# mesmo que as bibliotecas de IA n√£o estejam instaladas (o modo 'local' ainda ser√° funcional).
try:
    import openai
except ImportError:
    openai = None # Se falhar, a vari√°vel √© setada para None, desabilitando o modo 'openai'.

try:
    import google.generativeai as genai
except ImportError:
    genai = None # Se falhar, a vari√°vel √© setada para None, desabilitando o modo 'gemini'.

# --- Configura√ß√£o Inicial do Flask e do Modelo Local ---
# Inicializa a aplica√ß√£o Flask.
app = Flask(__name__)

# Define o caminho para o arquivo do modelo de ML local.
MODEL_PATH = "models/email_classifier.joblib"
# Vari√°vel para armazenar o modelo carregado.
model = None

# Bloco de inicializa√ß√£o: tenta carregar o modelo de ML na mem√≥ria quando a aplica√ß√£o inicia.
# Isso evita a necessidade de recarregar o modelo a cada requisi√ß√£o, melhorando a performance.
print("--- INICIALIZANDO APLICA√á√ÉO ---")
# Verifica se o arquivo do modelo existe no caminho especificado.
if os.path.exists(MODEL_PATH):
    try:
        # Carrega o modelo de ML usando joblib.
        model = joblib.load(MODEL_PATH)
        print(f"‚úÖ Modelo local carregado com sucesso de '{MODEL_PATH}'")
    except Exception as e:
        # Se ocorrer um erro durante o carregamento (ex: arquivo corrompido),
        # o erro √© logado e a vari√°vel 'model' permanece como None.
        print(f"‚ùå Erro ao carregar o modelo local: {e}")
        model = None
else:
    # Aviso caso o modelo n√£o seja encontrado no deploy. Isso pode acontecer se
    # o script de treinamento n√£o foi executado com sucesso durante o build.
    print(f"‚ö†Ô∏è  Aviso: Modelo local n√£o foi encontrado em '{MODEL_PATH}'. O modo 'local' n√£o funcionar√°.")
print("--- APLICA√á√ÉO PRONTA ---")


# --- Fun√ß√µes Auxiliares de Processamento e Heur√≠stica ---

def extract_text_from_pdf(file_stream):
    """
    Extrai o texto contido em um objeto de arquivo PDF (stream de bytes).

    Args:
        file_stream (io.BytesIO): Stream de bytes do arquivo PDF, vindo do request do Flask.

    Returns:
        str: Todo o texto extra√≠do do PDF, concatenado, ou uma string vazia em caso de erro.
    """
    try:
        # Cria um objeto leitor de PDF a partir do stream de bytes em mem√≥ria.
        reader = PyPDF2.PdfReader(file_stream)
        # Usa uma list comprehension para iterar sobre todas as p√°ginas,
        # extrair o texto de cada uma e juntar tudo em uma √∫nica string com quebras de linha.
        text_pages = [page.extract_text() for page in reader.pages if page.extract_text()]
        return "\n".join(text_pages)
    except Exception as e:
        # Loga o erro de extra√ß√£o no console do servidor para debug.
        print(f"Erro ao extrair texto do PDF: {e}")
        return ""
    
def detect_order_info(text):
    """
    Detecta n√∫meros de pedido, nota fiscal ou identificadores longos no texto usando express√µes regulares.
    
    A presen√ßa de tais n√∫meros √© uma forte heur√≠stica para um e-mail 'Produtivo' (que requer a√ß√£o).

    Args:
        text (str): O corpo do e-mail.

    Returns:
        str or None: O n√∫mero de pedido encontrado ou None se nenhum for detectado.
    """
    text_low = text.lower()
    
    # Padr√£o 1: Busca por 'pedido' seguido opcionalmente por ':', espa√ßo ou '-' e, em seguida, pelo menos 4 d√≠gitos.
    # Ex: "pedido 1234", "pedido:5678", "pedido-9101"
    m = re.search(r'pedido[:\s-]*([0-9]{4,})', text_low)
    if m:
        return m.group(1) # Retorna o grupo de captura (apenas os d√≠gitos do pedido).
        
    # Padr√£o 2: Busca por uma sequ√™ncia isolada de 6 ou mais d√≠gitos.
    # O `\b` significa "word boundary" (fronteira de palavra), garantindo que n√£o pegamos
    # parte de um n√∫mero maior, como um CEP ou telefone.
    # Ex: "ID 123456", "rastreio 987654321"
    m2 = re.search(r'\b([0-9]{6,})\b', text_low)
    if m2:
        return m2.group(1)
        
    return None # Retorna None se nenhum padr√£o for encontrado.


# <<< IN√çCIO DA MELHORIA DO GPT >>>
def template_reply(label, text=""):
    """
    Gera uma resposta autom√°tica padronizada com base no r√≥tulo de classifica√ß√£o
    e ajustada por heur√≠stica para e-mails 'Produtivos'.

    üîß Melhorias: respostas contextuais para anexos, comunica√ß√£o formal e termos acad√™micos.
    """
    text_lower = text.lower()
    order_id = detect_order_info(text)

    # L√≥gica para e-mails que requerem a√ß√£o (Produtivo)
    if label == "Produtivo":
        # Caso 1: Se um n√∫mero de pedido foi detectado.
        if order_id:
            return (f"Ol√°! Obrigado pelo contato. "
                    f"Identificamos o pedido {order_id} em sua mensagem. "
                    "Estamos verificando e retornaremos com a nota fiscal ou atualiza√ß√£o do status em breve.")
        
        # Caso 2: Se palavras-chave de nota fiscal forem encontradas.
        if any(k in text_lower for k in ["nota fiscal", "nota-fiscal", "nf-e", "nota_fiscal"]):
            return ("Ol√°! Obrigado pelo contato. "
                    "Vamos verificar a nota fiscal solicitada e retornaremos assim que poss√≠vel.")
        
        # Caso 3: Se palavras-chave de cancelamento/devolu√ß√£o forem encontradas.
        if any(k in text_lower for k in ["cancelamento", "devolu√ß√£o", "reembolso", "assinatura"]):
            return ("Ol√°! Recebemos sua solicita√ß√£o. "
                    "Nossa equipe est√° analisando e retornar√° em breve.")

        # üîß Caso 4: Se houver anexos ou materiais (slides, curr√≠culo, documento).
        if any(k in text_lower for k in ["anexo", "slides", "curr√≠culo", "documento"]):
            return ("Prezado(a), obrigado pelo envio do material. "
                    "Ele ser√° muito √∫til e j√° estamos organizando para utiliza√ß√£o.")

        # üîß Caso 5: Se for comunica√ß√£o acad√™mica/profissional.
        if any(k in text_lower for k in ["professor", "aluno", "disciplina", "tarefa", "projeto", "atividade"]):
            return ("Prezado(a) Professor(a), agradecemos a mensagem e o envio. "
                    "Estamos acompanhando com aten√ß√£o.")

        # üîß Caso 6: Comunica√ß√£o formal (prezado/atenciosamente).
        if any(k in text_lower for k in ["prezado", "atenciosamente"]):
            return ("Agradecemos o contato e confirmamos o recebimento da sua mensagem. "
                    "Nossa equipe est√° √† disposi√ß√£o.")
        
        # Caso gen√©rico se nada se aplicar.
        return ("Ol√°! Recebemos sua solicita√ß√£o e vamos analisar. "
                "Por favor, confirme o n√∫mero do seu pedido ou envie mais detalhes.")
    
    # L√≥gica para e-mails que n√£o requerem a√ß√£o imediata (Improdutivo)
    else:
        return ("Ol√°! Agradecemos a sua mensagem. "
                "Entraremos em contato se for necess√°ria alguma a√ß√£o. "
                "Tenha um √≥timo dia!")
# <<< FIM DA MELHORIA DO GPT >>>


# --- Fun√ß√µes de Classifica√ß√£o com Modelos de IA ---

# --- FUN√á√ÉO ORIGINAL MANTIDA ---
def classify_with_gemini(text, api_key):
    """
    Classifica o e-mail e gera resposta usando a API do Gemini.
    
    O modelo √© instru√≠do a retornar um JSON estruturado para facilitar o parsing.

    Args:
        text (str): O corpo do e-mail.
        api_key (str): Chave de API do Google Gemini.

    Returns:
        tuple: (label, confidence, reply) ou ("Erro", 0.0, mensagem de erro).
    """
    if genai is None:
        return "Erro", 0.0, "A biblioteca do Google Gemini n√£o est√° instalada. Rode: pip install google-generativeai"

    try:
        # Configura a chave de API para a sess√£o.
        genai.configure(api_key=api_key)

        # Inicializa o modelo Gemini (gemini-1.5-flash) com baixa temperatura para respostas mais determin√≠sticas.
        model_gemini = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config={"temperature": 0}
        )

        # Prompt de engenharia: Define o papel do assistente e a estrutura de sa√≠da JSON.
        prompt = (
            "Voc√™ √© um assistente de e-mail. Classifique o e-mail a seguir em 'Produtivo' ou 'Improdutivo' "
            "e gere uma resposta autom√°tica apropriada. Retorne SOMENTE um JSON v√°lido com os campos: "
            "'label' (string), 'confidence' (float entre 0.0 e 1.0), e 'reply' (string). "
            f"E-mail para an√°lise: '''{text}'''"
        )

        # Faz a chamada √† API.
        response = model_gemini.generate_content(prompt)

        # Trata as diferentes formas de extrair o texto da resposta (por seguran√ßa).
        if hasattr(response, "text") and response.text:
            content = response.text
        elif hasattr(response, "candidates") and response.candidates:
            content = response.candidates[0].content.parts[0].text
        else:
            raise ValueError("N√£o foi poss√≠vel extrair o texto da resposta do Gemini.")

        # Limpa o texto da resposta (remove '```json' e '```' que a IA pode incluir) e converte para JSON.
        content = content.strip().lstrip("```json").rstrip("```")
        parsed = json.loads(content)

        # Extrai os valores do JSON e garante que a confian√ßa seja um float.
        return parsed.get("label"), float(parsed.get("confidence", 0)), parsed.get("reply", "")

    except Exception as e:
        print(f"Erro na API do Gemini: {e}")
        # Retorna erro amig√°vel em caso de falha na API.
        return "Erro", 0.0, f"Ocorreu um erro ao comunicar com a API do Gemini: {e}"

# --- FUN√á√ÉO ORIGINAL MANTIDA ---
def classify_with_openai(text, api_key):
    """
    Classifica o e-mail e gera resposta usando a API da OpenAI (GPT).

    Args:
        text (str): O corpo do e-mail.
        api_key (str): Chave de API da OpenAI.

    Returns:
        tuple: (label, confidence, reply) ou ("Erro", 0.0, mensagem de erro).
    """
    if openai is None:
        return "Erro", 0.0, "A biblioteca da OpenAI n√£o est√° instalada. Rode: pip install openai"

    try:
        # Configura a chave de API.
        # NOTA: O SDK mais recente da OpenAI pode usar `client = OpenAI(api_key=api_key)`
        # mas esta vers√£o ainda usa a atribui√ß√£o direta.
        openai.api_key = api_key

        # Prompt de engenharia: Semelhante ao Gemini, define o papel e a estrutura JSON.
        prompt = (
            "Voc√™ √© um assistente de e-mail. Classifique o e-mail a seguir em 'Produtivo' ou 'Improdutivo' "
            "e gere uma resposta autom√°tica apropriada. Retorne SOMENTE um JSON v√°lido com os campos: "
            "'label' (string), 'confidence' (float entre 0.0 e 1.0), e 'reply' (string). "
            f"E-mail para an√°lise: '''{text}'''"
        )

        # Faz a chamada √† API de Chat Completion.
        resp = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0 # Baixa temperatura para resultados mais consistentes.
        )

        # Extrai o conte√∫do da resposta e converte para JSON.
        content = resp['choices'][0]['message']['content']
        parsed = json.loads(content)
        
        # Extrai e retorna os resultados.
        return parsed.get("label"), float(parsed.get("confidence", 0)), parsed.get("reply", "")

    except Exception as e:
        print(f"Erro na API da OpenAI: {e}")
        # Retorna erro amig√°vel em caso de falha na API.
        return "Erro", 0.0, f"Ocorreu um erro ao comunicar com a API da OpenAI: {e}"


# <<< IN√çCIO DA MELHORIA DO GPT >>>
def classify_local(text):
    """
    Usa o modelo de Machine Learning local para classifica√ß√£o,
    aplicando heur√≠sticas e thresholds de confian√ßa.

    üîß Melhorias: 
    - Threshold ajustado para >=0.6 (Produtivo).
    - Zona de incerteza entre 0.5‚Äì0.6 com aviso.
    - Refor√ßo para termos acad√™micos e anexos.
    - **INCREMENTO:** A l√≥gica de confian√ßa agora reflete a probabilidade da classe predita.
    """
    if model is None:
        # Verifica se o modelo foi carregado com sucesso na inicializa√ß√£o.
        return "Erro", 0.0, "O modelo local n√£o est√° carregado. Treine o modelo primeiro."

    try:
        # Calcula a probabilidade de pertencer √† classe 'Produtivo'.
        # Assume que 'Produtivo' √© a segunda classe (√≠ndice 1).
        proba_prod = float(model.predict_proba([text])[0][1])

        # Heur√≠stica de Refor√ßo: Se detectar um n√∫mero de pedido/nota, 
        # a confian√ßa na classe 'Produtivo' √© elevada.
        if detect_order_info(text):
            # Aumenta a probabilidade para, no m√≠nimo, 85%.
            proba_prod = max(proba_prod, 0.85)

        # üîß Heur√≠stica adicional: anexos, slides, termos acad√™micos.
        if any(k in text.lower() for k in ["anexo", "slides", "professor", "disciplina", "curr√≠culo", "documento"]):
            proba_prod = max(proba_prod, 0.8)

        # --- L√ìGICA DE DECIS√ÉO E CONFIAN√áA CORRIGIDA ---
        # Aplica os Thresholds (Limiares) de Decis√£o.
        if proba_prod >= 0.6:
            # Alta confian√ßa em Produtivo.
            label = "Produtivo"
            # A confian√ßa a ser exibida √© a probabilidade da classe 'Produtivo'.
            confidence = proba_prod
        # Se a probabilidade de ser 'Produtivo' for menor que 0.6...
        else:
            # A classifica√ß√£o final √© 'Improdutivo'.
            label = "Improdutivo"
            # A confian√ßa a ser exibida √© a probabilidade da classe 'Improdutivo',
            # que √© calculada como 1 menos a probabilidade de ser 'Produtivo'.
            confidence = 1 - proba_prod
            
            # Condi√ß√£o para a Zona de Incerteza (entre 50% e 60%):
            # A l√≥gica de decis√£o j√° classifica como 'Improdutivo', aqui apenas adicionamos o aviso.
            if proba_prod >= 0.5:
                print("Aviso: classifica√ß√£o incerta (zona 0.5‚Äì0.6), revis√£o humana sugerida.")
        
        # Gera a resposta autom√°tica usando a fun√ß√£o auxiliar com o r√≥tulo j√° definido.
        reply = template_reply(label, text)
        return label, confidence, reply
        
    except Exception as e:
        print(f"Erro no modelo local: {e}")
        return "Erro", 0.0, "Ocorreu um erro ao usar o modelo local."
# <<< FIM DA MELHORIA DO GPT >>>


# --- Rotas da Aplica√ß√£o Flask ---

@app.route("/", methods=["GET"])
def index():
    """
    Rota principal (GET). 
    Apenas renderiza o template 'index.html' (a interface do usu√°rio).
    """
    return render_template("index.html")


@app.route("/process", methods=["POST"])
def process():
    """
    Rota de processamento (POST). 
    Recebe os dados do formul√°rio (texto, modo, chave de API, arquivo)
    e executa a classifica√ß√£o de acordo com o modo escolhido.
    Esta fun√ß√£o foi atualizada para corrigir bugs de UX.
    """
    try:
        # 1. Coleta e Sanitiza os Dados do Formul√°rio
        text_input = request.form.get("text_input", "").strip()
        mode = request.form.get("mode", "local")
        api_key = request.form.get("api_key", "").strip()
        file = request.files.get("file")

        # Inicializa a vari√°vel de texto principal como None.
        # Isso √© parte da nova l√≥gica de prioridade de entrada.
        text = None
        
        # ### IN√çCIO DA CORRE√á√ÉO BUG 2: PRIORIDADE DO ARQUIVO ###
        # A l√≥gica agora prioriza o conte√∫do de um arquivo enviado sobre o texto digitado.
        # Isso evita que o usu√°rio tenha que apagar o texto da caixa para analisar um arquivo.
        
        # Primeiro, verifica se um objeto 'file' foi enviado e se ele tem um nome de arquivo.
        if file and file.filename:
            filename = file.filename.lower()
            
            print(f"Arquivo '{filename}' recebido, processando...")
            
            # Se for um arquivo PDF, chama a fun√ß√£o de extra√ß√£o de texto de PDF.
            if filename.endswith(".pdf"):
                # Passa o arquivo como um stream de bytes em mem√≥ria.
                text = extract_text_from_pdf(io.BytesIO(file.read()))
            
            # Se for um arquivo de texto, l√™ seu conte√∫do.
            elif filename.endswith(".txt"):
                # Decodifica o conte√∫do do arquivo como UTF-8, ignorando erros de codifica√ß√£o.
                text = file.read().decode("utf-8", errors="ignore")

        # Se, ap√≥s a verifica√ß√£o do arquivo, a vari√°vel 'text' ainda estiver vazia ou None,
        # ent√£o usamos o conte√∫do da caixa de texto como a fonte de dados.
        if not text:
            text = text_input
        # ### FIM DA CORRE√á√ÉO BUG 2 ###

        # 3. Valida√ß√£o do Texto Final
        # Verifica se, ap√≥s todas as l√≥gicas, existe algum texto para analisar.
        if not text:
            # ### IN√çCIO DA CORRE√á√ÉO BUG 1: PERSIST√äNCIA DA SELE√á√ÉO DO MODELO ###
            # Ao renderizar a p√°gina com um erro, agora passamos a vari√°vel 'selected_mode'.
            # Isso dir√° ao template Jinja2 para manter a op√ß√£o do dropdown que o usu√°rio escolheu,
            # melhorando a experi√™ncia do usu√°rio.
            return render_template("index.html", error="Nenhum texto ou arquivo v√°lido foi enviado.", selected_mode=mode)
            # ### FIM DA CORRE√á√ÉO BUG 1 ###

        # 4. Execu√ß√£o da Classifica√ß√£o Baseada no Modo
        label, confidence, reply = "", 0.0, ""

        if mode == "local":
            label, confidence, reply = classify_local(text)
        
        elif mode == "gemini":
            # Usa a chave da interface ou busca na vari√°vel de ambiente.
            key_to_use = api_key or os.getenv("GEMINI_API_KEY")
            if not key_to_use:
                return render_template("index.html", error="Chave de API do Gemini n√£o fornecida na interface nem encontrada no ambiente.", selected_mode=mode)
            label, confidence, reply = classify_with_gemini(text, key_to_use)
            
        elif mode == "openai":
            # Usa a chave da interface ou busca na vari√°vel de ambiente.
            key_to_use = api_key or os.getenv("OPENAI_API_KEY")
            if not key_to_use:
                return render_template("index.html", error="Chave de API da OpenAI n√£o fornecida na interface nem encontrada no ambiente.", selected_mode=mode)
            label, confidence, reply = classify_with_openai(text, key_to_use)
            
        else:
            # Tratamento para modo inv√°lido (seguran√ßa).
            return render_template("index.html", error="Modo de opera√ß√£o inv√°lido selecionado.", selected_mode=mode)

        # 5. Tratamento de Erros de Classifica√ß√£o
        if label == "Erro":
            # Retorna o erro espec√≠fico da fun√ß√£o de classifica√ß√£o, tamb√©m persistindo o modo.
            return render_template("index.html", error=reply, original_text=text, selected_mode=mode)

        # 6. Renderiza√ß√£o de Resultados
        # ### IN√çCIO DA CORRE√á√ÉO BUG 1 (CASO DE SUCESSO) ###
        # No retorno de sucesso, tamb√©m passamos 'selected_mode=mode' para o template.
        # Isso garante que ap√≥s uma an√°lise bem-sucedida, o dropdown permane√ßa
        # na √∫ltima op√ß√£o utilizada pelo usu√°rio.
        return render_template("index.html",
                               original_text=text,
                               result_label=label,
                               confidence=confidence,
                               suggested_reply=reply,
                               selected_mode=mode)
        # ### FIM DA CORRE√á√ÉO BUG 1 ###

    except Exception as e:
        # Tratamento de erro geral para qualquer falha n√£o esperada na rota.
        print(f"Erro geral na rota /process: {e}")
        traceback.print_exc() # Imprime o stack trace para o console para debug.
        return render_template("index.html", error="Ocorreu um erro inesperado no servidor. Verifique o console para mais detalhes.")


# --- Execu√ß√£o da Aplica√ß√£o ---
if __name__ == "__main__":
    # Garante que o servidor Flask seja executado apenas quando o script for chamado diretamente.
    # O comando `flask run` tamb√©m pode ser usado se as vari√°veis de ambiente estiverem configuradas.
    
    # debug=True: Ativa o modo de depura√ß√£o.
    # Isso permite recarregamento autom√°tico do servidor quando o c√≥digo √© alterado
    # e exibe um console de depura√ß√£o interativo no navegador em caso de erro.
    # NUNCA use debug=True em um ambiente de produ√ß√£o real.
    
    # port=5000: Define a porta de execu√ß√£o padr√£o para o servidor de desenvolvimento.
    # O Render ignora esta configura√ß√£o e usa a porta que ele designa.
    app.run(debug=True, port=5000)